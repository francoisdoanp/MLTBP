{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francoisdoanp/MLTBP/blob/master/Project_MLv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "23I2qdWI4FFG"
      },
      "source": [
        "# **Machine learning - Final Project**\n",
        "\n",
        "# Turbofan engine degradation dataset (NASA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nOxT0nXc366_"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "**Importing necessary packages**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J_GboRpW2F6s",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import sklearn.metrics as metrics\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from keras import regularizers\n",
        "import keras.backend as k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qw2HZErj35aM"
      },
      "source": [
        "**Importing the Turbofan engine degradation dataset.**\n",
        "\n",
        "**Files are located in the following Github repository: https://github.com/francoisdoanp/MLTBP**\n",
        "\n",
        "We have 4 training datasets, which contains information about one hundred engines, all of the same type. Thus, we will combine the training and test data sets. \n",
        "\n",
        "The training and test sets have 21 columns: ID, Time (Cycles), 3 columns for operational settings and 21 sensor measurements.\n",
        "\n",
        "The training and testing sets have the same format, while the validation sets only contain the real RUL (remaining useful life).\n",
        "\n",
        "For more information on the data, consult the read me at the following address:https://github.com/francoisdoanp/MLTBP/blob/master/readme.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CmZzDVll4vDW",
        "colab": {}
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/francoisdoanp/MLTBP/master/'\n",
        "\n",
        "file_train_1 = 'train_FD001.txt'\n",
        "file_train_2 = 'train_FD002.txt'\n",
        "file_train_3 = 'train_FD003.txt'\n",
        "file_train_4 = 'train_FD004.txt'\n",
        "\n",
        "file_test_1 = 'test_FD001.txt'\n",
        "file_test_2 = 'test_FD002.txt'\n",
        "file_test_3 = 'test_FD003.txt'\n",
        "file_test_4 = 'test_FD004.txt'\n",
        "\n",
        "file_valid_1 = 'RUL_FD001.txt'\n",
        "file_valid_2 = 'RUL_FD002.txt'\n",
        "file_valid_3 = 'RUL_FD003.txt'\n",
        "file_valid_4 = 'RUL_FD004.txt'\n",
        "\n",
        "\n",
        "pt1 = pd.read_csv(url_base + file_train_1, sep=' ', header=None)\n",
        "pt2 = pd.read_csv(url_base + file_train_2, sep=' ', header=None)\n",
        "pt3 = pd.read_csv(url_base + file_train_3, sep=' ', header=None)\n",
        "pt4 = pd.read_csv(url_base + file_train_4, sep=' ', header=None)\n",
        "\n",
        "pte1 = pd.read_csv(url_base + file_test_1, sep=' ', header=None)\n",
        "pte2 = pd.read_csv(url_base + file_test_2, sep=' ', header=None)\n",
        "pte3 = pd.read_csv(url_base + file_test_3, sep=' ', header=None)\n",
        "pte4 = pd.read_csv(url_base + file_test_4, sep=' ', header=None)\n",
        "\n",
        "pv1 = pd.read_csv(url_base + file_valid_1, header=None)\n",
        "pv2 = pd.read_csv(url_base + file_valid_2, header=None)\n",
        "pv3 = pd.read_csv(url_base + file_valid_3, header=None)\n",
        "pv4 = pd.read_csv(url_base + file_valid_4, header=None)\n",
        "\n",
        "\n",
        "# Updating ids\n",
        "\n",
        "pt2[0] = pt2[0].apply(lambda x: x+100)\n",
        "pt3[0] = pt3[0].apply(lambda x: x+360)\n",
        "pt4[0] = pt4[0].apply(lambda x: x+460)\n",
        "\n",
        "pte2[0] = pte2[0].apply(lambda x: x+100)\n",
        "pte3[0] = pte3[0].apply(lambda x: x+359)\n",
        "pte4[0] = pte4[0].apply(lambda x: x+459)\n",
        "\n",
        "\n",
        "# Joining the dataframes\n",
        "\n",
        "train_pd = pd.concat([pt1,pt2,pt3,pt4]).reset_index(drop=True)\n",
        "test_pd = pd.concat([pte1,pte2,pte3,pte4]).reset_index(drop=True)\n",
        "valid_pd = pd.concat([pv1,pv2,pv3,pv4], ignore_index=True)\n",
        "\n",
        "train_pd = train_pd.drop(train_pd.columns[[26,27]], axis='columns')\n",
        "test_pd = test_pd.drop(test_pd.columns[[26,27]], axis='columns')\n",
        "\n",
        "\n",
        "# Assigning labels to Dataframe's columns based on the Readme\n",
        "\n",
        "train_pd.columns = ['id', 'Time (Cycles)', 'OS1', 'OS2', 'OS3', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21']\n",
        "test_pd.columns = ['id', 'Time (Cycles)', 'OS1', 'OS2', 'OS3', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21']\n",
        "valid_pd.columns = ['RUL']\n",
        "\n",
        "#Loading scaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G9va4qroPgeB"
      },
      "source": [
        "**Adding variables Conditons and fault mode**\n",
        "\n",
        "Note:\n",
        "\n",
        "**Condition (ONE)** and **Fault ONE** are binary variables.\n",
        "\n",
        "When Condition(ONE) = 1 (true), it means that the condition is at Sea Level\n",
        "\n",
        "When Condition(ONE) = 0 (false), it means NO, the condition IS NOT AT SEA LEVEL, and thus is the  second condition; SIX.\n",
        "\n",
        "When Fault ONE = 1 (true), it means that the fault modes is one (HPC Degradation)\n",
        "\n",
        "When Fault ONE = 0 (false), it means that the fault mode is TWO (HPC Degradation and Fan degradation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WQaAK97PfZU",
        "colab": {}
      },
      "source": [
        "# Adding variables Condition and fault modes\n",
        "\n",
        "def value_condition_train(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 360) & (row['id'] > 100):\n",
        "    return 0\n",
        "  elif (row['id'] <= 460) & (row['id'] > 360):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def value_fault_train(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 360) & (row['id'] > 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 460) & (row['id'] > 360):\n",
        "    return 0\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def value_condition_test(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 359) & (row['id'] > 100):\n",
        "    return 0\n",
        "  elif (row['id'] <= 459) & (row['id'] > 359):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def value_fault_test(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 359) & (row['id'] > 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 459) & (row['id'] > 359):\n",
        "    return 0\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "train_pd['Condition (One)'] = train_pd.apply(value_condition_train, axis=1)\n",
        "train_pd['Fault ONE'] = train_pd.apply(value_fault_train,axis=1)\n",
        "\n",
        "test_pd['Condition (One)'] = test_pd.apply(value_condition_test, axis=1)\n",
        "test_pd['Fault ONE'] = test_pd.apply(value_fault_test,axis=1)\n",
        "\n",
        "display(train_pd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lAl6DjVwhdhU"
      },
      "source": [
        "At this stage, we create the truth remaining useful life (RUL) for the training set.\n",
        "\n",
        "Important note: In the training set, the last Cycle (represented in the table by 'Time (Cycles)') is when the engine is considered unusable. However, in the test set, the last cycle IS NOT when the engine is considered unusable. It will fail at a later time. Thus, in the valid_pd, we have the true RUL. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UdbuXXApd51H",
        "colab": {}
      },
      "source": [
        "#Adding column for remaining useful life (RUL)\n",
        "\n",
        "y_train = pd.DataFrame(train_pd.groupby(['id'])['Time (Cycles)'].max())\n",
        "\n",
        "train_pd = pd.merge(train_pd,y_train, on='id')\n",
        "train_pd['RUL'] = train_pd['Time (Cycles)_y'] - train_pd['Time (Cycles)_x']\n",
        "train_pd = train_pd.drop('Time (Cycles)_y',1)\n",
        "train_pd = train_pd.rename(columns = {'Time (Cycles)_x':'Time (Cycles)'})\n",
        "\n",
        "y_train = train_pd.iloc[:,28]\n",
        "y_train_id = train_pd.iloc[:,[0,28]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vhz30Roasqsx",
        "colab": {}
      },
      "source": [
        "def model_score(y_true, y_pred):\n",
        "  pred_df = pd.DataFrame(y_pred)\n",
        "  test_err = pd.concat([y_true,pred_df], axis=1, ignore_index=True)\n",
        "  test_err.columns = ['RUL', 'Pred_RUL']\n",
        "  a1 = 10\n",
        "  a2 = 13\n",
        "  score=0\n",
        "\n",
        "  for index, row in test_err.iterrows():\n",
        "    d = row['Pred_RUL'] - row['RUL']\n",
        "    if d < 0:\n",
        "      score += np.expm1(-(d/a1))-1\n",
        "    else:\n",
        "      score += np.expm1(d/a2)-1\n",
        "\n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-mn7nZq39QVV"
      },
      "source": [
        "**Preparing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSvBTo9285u0",
        "colab": {}
      },
      "source": [
        "# Scaling data\n",
        "\n",
        "train_pd_scaled = train_pd.copy()\n",
        "train_pd_scaled.iloc[:,2:26] = scaler.fit_transform(train_pd.iloc[:,2:26])\n",
        "\n",
        "test_pd_scaled = test_pd.copy()\n",
        "test_pd_scaled.iloc[:,2:26] = scaler.fit_transform(test_pd.iloc[:,2:26])\n",
        "\n",
        "pd.set_option('display.max_columns', None)  \n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "print(train_pd_scaled)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twxKJeU5vc19",
        "colab_type": "text"
      },
      "source": [
        "# **Dataset with feature engineering (Rolling Average)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0HZDgSJvZqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_RA_pd = train_pd_scaled.copy()\n",
        "period_length=10\n",
        "\n",
        "\n",
        "# Rolling average for training data\n",
        "\n",
        "rolling_avg1 = train_pd_scaled.groupby('id')[\"OS1\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"OS1_rolling_avg\"] = rolling_avg1\n",
        "\n",
        "rolling_avg2 = train_pd_scaled.groupby('id')[\"OS2\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"OS2_rolling_avg\"] = rolling_avg2\n",
        "\n",
        "rolling_avg3 = train_pd_scaled.groupby('id')[\"OS3\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"OS3_rolling_avg\"] = rolling_avg3\n",
        "\n",
        "rolling_avg4 = train_pd_scaled.groupby('id')[\"S1\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S1_rolling_avg\"] = rolling_avg4\n",
        "\n",
        "rolling_avg5 = train_pd_scaled.groupby('id')[\"S2\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S2_rolling_avg\"] = rolling_avg5\n",
        "\n",
        "rolling_avg6 = train_pd_scaled.groupby('id')[\"S3\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S3_rolling_avg\"] = rolling_avg6\n",
        "\n",
        "rolling_avg7 = train_pd_scaled.groupby('id')[\"S4\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S4_rolling_avg\"] = rolling_avg7\n",
        "\n",
        "rolling_avg8 = train_pd_scaled.groupby('id')[\"S5\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S5_rolling_avg\"] = rolling_avg8\n",
        "\n",
        "rolling_avg9 = train_pd_scaled.groupby('id')[\"S6\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S6_rolling_avg\"] = rolling_avg9\n",
        "\n",
        "rolling_avg10 = train_pd_scaled.groupby('id')[\"S7\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S7_rolling_avg\"] = rolling_avg10\n",
        "\n",
        "rolling_avg11 = train_pd_scaled.groupby('id')[\"S8\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S8_rolling_avg\"] = rolling_avg11\n",
        "\n",
        "rolling_avg12 = train_pd_scaled.groupby('id')[\"S9\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S9_rolling_avg\"] = rolling_avg12\n",
        "\n",
        "rolling_avg13 = train_pd_scaled.groupby('id')[\"S10\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S10_rolling_avg\"] = rolling_avg13\n",
        "\n",
        "rolling_avg14 = train_pd_scaled.groupby('id')[\"S11\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S11_rolling_avg\"] = rolling_avg14\n",
        "\n",
        "rolling_avg15 = train_pd_scaled.groupby('id')[\"S12\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S12_rolling_avg\"] = rolling_avg15\n",
        "\n",
        "rolling_avg16 = train_pd_scaled.groupby('id')[\"S13\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S13_rolling_avg\"] = rolling_avg16\n",
        "\n",
        "rolling_avg17 = train_pd_scaled.groupby('id')[\"S14\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S14_rolling_avg\"] = rolling_avg17\n",
        "\n",
        "rolling_avg18 = train_pd_scaled.groupby('id')[\"S15\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S15_rolling_avg\"] = rolling_avg18\n",
        "\n",
        "rolling_avg19 = train_pd_scaled.groupby('id')[\"S16\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S16_rolling_avg\"] = rolling_avg19\n",
        "\n",
        "rolling_avg20 = train_pd_scaled.groupby('id')[\"S17\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S17_rolling_avg\"] = rolling_avg20\n",
        "\n",
        "rolling_avg21 = train_pd_scaled.groupby('id')[\"S18\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S18_rolling_avg\"] = rolling_avg21\n",
        "\n",
        "rolling_avg22 = train_pd_scaled.groupby('id')[\"S19\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S19_rolling_avg\"] = rolling_avg22\n",
        "\n",
        "rolling_avg23 = train_pd_scaled.groupby('id')[\"S20\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S20_rolling_avg\"] = rolling_avg23\n",
        "\n",
        "rolling_avg24 = train_pd_scaled.groupby('id')[\"S21\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "train_RA_pd[\"S21_rolling_avg\"] = rolling_avg24\n",
        "\n",
        "train_RA_pd.dropna(inplace=True)\n",
        "\n",
        "# Rolling average for test data\n",
        "\n",
        "test_RA_pd = test_pd_scaled.copy()\n",
        "period_length=10\n",
        "\n",
        "\n",
        "rolling_avg1 = test_pd_scaled.groupby('id')[\"OS1\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"OS1_rolling_avg\"] = rolling_avg1\n",
        "\n",
        "rolling_avg2 = test_pd_scaled.groupby('id')[\"OS2\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"OS2_rolling_avg\"] = rolling_avg2\n",
        "\n",
        "rolling_avg3 = test_pd_scaled.groupby('id')[\"OS3\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"OS3_rolling_avg\"] = rolling_avg3\n",
        "\n",
        "rolling_avg4 = test_pd_scaled.groupby('id')[\"S1\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S1_rolling_avg\"] = rolling_avg4\n",
        "\n",
        "rolling_avg5 = test_pd_scaled.groupby('id')[\"S2\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S2_rolling_avg\"] = rolling_avg5\n",
        "\n",
        "rolling_avg6 = test_pd_scaled.groupby('id')[\"S3\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S3_rolling_avg\"] = rolling_avg6\n",
        "\n",
        "rolling_avg7 = test_pd_scaled.groupby('id')[\"S4\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S4_rolling_avg\"] = rolling_avg7\n",
        "\n",
        "rolling_avg8 = test_pd_scaled.groupby('id')[\"S5\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S5_rolling_avg\"] = rolling_avg8\n",
        "\n",
        "rolling_avg9 = test_pd_scaled.groupby('id')[\"S6\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S6_rolling_avg\"] = rolling_avg9\n",
        "\n",
        "rolling_avg10 = test_pd_scaled.groupby('id')[\"S7\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S7_rolling_avg\"] = rolling_avg10\n",
        "\n",
        "rolling_avg11 = test_pd_scaled.groupby('id')[\"S8\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S8_rolling_avg\"] = rolling_avg11\n",
        "\n",
        "rolling_avg12 = test_pd_scaled.groupby('id')[\"S9\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S9_rolling_avg\"] = rolling_avg12\n",
        "\n",
        "rolling_avg13 = test_pd_scaled.groupby('id')[\"S10\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S10_rolling_avg\"] = rolling_avg13\n",
        "\n",
        "rolling_avg14 = test_pd_scaled.groupby('id')[\"S11\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S11_rolling_avg\"] = rolling_avg14\n",
        "\n",
        "rolling_avg15 = test_pd_scaled.groupby('id')[\"S12\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S12_rolling_avg\"] = rolling_avg15\n",
        "\n",
        "rolling_avg16 = test_pd_scaled.groupby('id')[\"S13\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S13_rolling_avg\"] = rolling_avg16\n",
        "\n",
        "rolling_avg17 = test_pd_scaled.groupby('id')[\"S14\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S14_rolling_avg\"] = rolling_avg17\n",
        "\n",
        "rolling_avg18 = test_pd_scaled.groupby('id')[\"S15\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S15_rolling_avg\"] = rolling_avg18\n",
        "\n",
        "rolling_avg19 = test_pd_scaled.groupby('id')[\"S16\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S16_rolling_avg\"] = rolling_avg19\n",
        "\n",
        "rolling_avg20 = test_pd_scaled.groupby('id')[\"S17\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S17_rolling_avg\"] = rolling_avg20\n",
        "\n",
        "rolling_avg21 = test_pd_scaled.groupby('id')[\"S18\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S18_rolling_avg\"] = rolling_avg21\n",
        "\n",
        "rolling_avg22 = test_pd_scaled.groupby('id')[\"S19\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S19_rolling_avg\"] = rolling_avg22\n",
        "\n",
        "rolling_avg23 = test_pd_scaled.groupby('id')[\"S20\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S20_rolling_avg\"] = rolling_avg23\n",
        "\n",
        "rolling_avg24 = test_pd_scaled.groupby('id')[\"S21\"] \\\n",
        "    .apply(lambda x: x.shift().rolling(period_length, min_periods=10)\n",
        "    .mean())\n",
        "test_RA_pd[\"S21_rolling_avg\"] = rolling_avg24\n",
        "\n",
        "test_RA_pd.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOxq5ftzvn5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing RUL and id columns, as we do not want these features to be in our predictors\n",
        "\n",
        "train_pd_ra = train_RA_pd.copy()\n",
        "train_pd_ra = train_pd_ra.drop(['id', 'RUL','Time (Cycles)'],axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ENkana6AH9LA",
        "colab": {}
      },
      "source": [
        "# Removing RUL and id columns, as we do not want these features to be in our predictors\n",
        "\n",
        "train_pd_lm = train_pd_scaled.copy()\n",
        "train_pd_lm = train_pd_lm.drop(['id', 'RUL', 'Time (Cycles)'],axis=1)\n",
        "\n",
        "# Keeping only last time cycle for each id\n",
        "\n",
        "idx = test_pd_scaled.groupby(['id'])['Time (Cycles)'].transform(max) == test_pd_scaled['Time (Cycles)']\n",
        "test_pd_lm = test_pd_scaled[idx]\n",
        "\n",
        "idx4 = test_RA_pd.groupby(['id'])['Time (Cycles)'].transform(max) == test_RA_pd['Time (Cycles)']\n",
        "test_pd_ra = test_RA_pd[idx4]\n",
        "\n",
        "# Removing id column\n",
        "\n",
        "test_pd_lm = test_pd_lm.drop(['id', 'Time (Cycles)'], axis=1)\n",
        "test_pd_ra = test_pd_ra.drop(['id', 'Time (Cycles)'], axis=1)\n",
        "\n",
        "y_train_ra = y_train_id.groupby('id').apply(lambda group: group.iloc[10:]).reset_index(drop=True)\n",
        "y_train_ra = y_train_ra.drop(['id'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhYn-0V5kSw9",
        "colab_type": "text"
      },
      "source": [
        "# **Model 3: Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFTyU01LH4SK",
        "colab_type": "text"
      },
      "source": [
        "# **Model 3.1: Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvWvliCAHwKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building Model\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(100,  activation='relu', input_shape=[len(train_pd_lm.keys())], kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01),  activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model2.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model2.add(Dense(1))\n",
        "\n",
        "model2.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model2.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2XwNbZeKOlv",
        "colab_type": "code",
        "outputId": "057c9801-e12a-41b2-ebf4-aa12c913a07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fitting model\n",
        "\n",
        "NNmodel2 = model2.fit(train_pd_lm, y_train, epochs=100, batch_size=64, validation_split=0.05, verbose=1,callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')])\n",
        "#NNmodel2 = model2.fit(train_pd_lm, y_train, epochs=10, batch_size=200, validation_split=0.05, verbose=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 152341 samples, validate on 8018 samples\n",
            "Epoch 1/500\n",
            "152341/152341 [==============================] - 10s 63us/step - loss: 5051.5475 - mean_absolute_error: 46.5742 - val_loss: 3431.8512 - val_mean_absolute_error: 40.4647\n",
            "Epoch 2/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3464.6898 - mean_absolute_error: 40.9135 - val_loss: 3495.5296 - val_mean_absolute_error: 43.5367\n",
            "Epoch 3/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3277.7418 - mean_absolute_error: 40.0796 - val_loss: 3662.7956 - val_mean_absolute_error: 43.9263\n",
            "Epoch 4/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3185.6948 - mean_absolute_error: 39.6270 - val_loss: 3416.0031 - val_mean_absolute_error: 43.8641\n",
            "Epoch 5/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 3120.9436 - mean_absolute_error: 39.2643 - val_loss: 3882.3932 - val_mean_absolute_error: 50.1131\n",
            "Epoch 6/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3085.8113 - mean_absolute_error: 39.0520 - val_loss: 3961.0013 - val_mean_absolute_error: 48.0481\n",
            "Epoch 7/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3068.2487 - mean_absolute_error: 38.9685 - val_loss: 3272.5957 - val_mean_absolute_error: 42.1660\n",
            "Epoch 8/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 3035.7411 - mean_absolute_error: 38.7552 - val_loss: 3424.3812 - val_mean_absolute_error: 44.8880\n",
            "Epoch 9/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3024.1157 - mean_absolute_error: 38.6924 - val_loss: 3331.0615 - val_mean_absolute_error: 44.3351\n",
            "Epoch 10/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 3006.6613 - mean_absolute_error: 38.5721 - val_loss: 3390.5759 - val_mean_absolute_error: 45.6997\n",
            "Epoch 11/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2992.0415 - mean_absolute_error: 38.4937 - val_loss: 2801.9185 - val_mean_absolute_error: 38.6538\n",
            "Epoch 12/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2983.3561 - mean_absolute_error: 38.4123 - val_loss: 3313.5250 - val_mean_absolute_error: 42.8134\n",
            "Epoch 13/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2970.8379 - mean_absolute_error: 38.3348 - val_loss: 3174.1176 - val_mean_absolute_error: 44.0515\n",
            "Epoch 14/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2967.7043 - mean_absolute_error: 38.2768 - val_loss: 3001.1201 - val_mean_absolute_error: 41.3642\n",
            "Epoch 15/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2960.9799 - mean_absolute_error: 38.2477 - val_loss: 3364.9078 - val_mean_absolute_error: 44.8565\n",
            "Epoch 16/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2952.6426 - mean_absolute_error: 38.2058 - val_loss: 3590.5589 - val_mean_absolute_error: 46.3055\n",
            "Epoch 17/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2941.6221 - mean_absolute_error: 38.1391 - val_loss: 3342.1488 - val_mean_absolute_error: 44.5805\n",
            "Epoch 18/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2944.3374 - mean_absolute_error: 38.1481 - val_loss: 3020.6258 - val_mean_absolute_error: 39.1733\n",
            "Epoch 19/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2933.0672 - mean_absolute_error: 38.0593 - val_loss: 2953.2282 - val_mean_absolute_error: 40.4672\n",
            "Epoch 20/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2932.8113 - mean_absolute_error: 38.0191 - val_loss: 3056.4563 - val_mean_absolute_error: 40.8244\n",
            "Epoch 21/500\n",
            "152341/152341 [==============================] - 9s 61us/step - loss: 2924.9077 - mean_absolute_error: 38.0345 - val_loss: 3150.8318 - val_mean_absolute_error: 42.6548\n",
            "Epoch 22/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2925.1485 - mean_absolute_error: 38.0001 - val_loss: 3056.0391 - val_mean_absolute_error: 42.7228\n",
            "Epoch 23/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2916.0696 - mean_absolute_error: 37.9190 - val_loss: 3316.0069 - val_mean_absolute_error: 43.9105\n",
            "Epoch 24/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2913.3183 - mean_absolute_error: 37.9195 - val_loss: 3244.0061 - val_mean_absolute_error: 43.7820\n",
            "Epoch 25/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2908.1370 - mean_absolute_error: 37.8799 - val_loss: 2762.5441 - val_mean_absolute_error: 38.2902\n",
            "Epoch 26/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2909.8527 - mean_absolute_error: 37.9003 - val_loss: 2871.0385 - val_mean_absolute_error: 38.6911\n",
            "Epoch 27/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2909.3865 - mean_absolute_error: 37.8497 - val_loss: 3137.7603 - val_mean_absolute_error: 42.8768\n",
            "Epoch 28/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2902.4689 - mean_absolute_error: 37.8345 - val_loss: 3014.3341 - val_mean_absolute_error: 40.7917\n",
            "Epoch 29/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2903.2561 - mean_absolute_error: 37.8357 - val_loss: 3047.1851 - val_mean_absolute_error: 41.0048\n",
            "Epoch 30/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2898.9907 - mean_absolute_error: 37.8394 - val_loss: 3083.3636 - val_mean_absolute_error: 41.4999\n",
            "Epoch 31/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2898.2732 - mean_absolute_error: 37.7778 - val_loss: 3116.5183 - val_mean_absolute_error: 41.7942\n",
            "Epoch 32/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2895.6600 - mean_absolute_error: 37.7207 - val_loss: 3154.9836 - val_mean_absolute_error: 43.6541\n",
            "Epoch 33/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2892.3708 - mean_absolute_error: 37.7060 - val_loss: 3337.7770 - val_mean_absolute_error: 44.2693\n",
            "Epoch 34/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2892.9871 - mean_absolute_error: 37.7175 - val_loss: 2859.5203 - val_mean_absolute_error: 40.3933\n",
            "Epoch 35/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2890.8265 - mean_absolute_error: 37.7321 - val_loss: 2997.9337 - val_mean_absolute_error: 40.7058\n",
            "Epoch 36/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2887.3002 - mean_absolute_error: 37.7227 - val_loss: 2899.9044 - val_mean_absolute_error: 39.7137\n",
            "Epoch 37/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2884.3192 - mean_absolute_error: 37.6789 - val_loss: 2974.0206 - val_mean_absolute_error: 40.7226\n",
            "Epoch 38/500\n",
            "152341/152341 [==============================] - 9s 57us/step - loss: 2888.3868 - mean_absolute_error: 37.6929 - val_loss: 3151.6296 - val_mean_absolute_error: 42.0850\n",
            "Epoch 39/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2879.9357 - mean_absolute_error: 37.6283 - val_loss: 2764.8785 - val_mean_absolute_error: 38.4481\n",
            "Epoch 40/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2884.5858 - mean_absolute_error: 37.6780 - val_loss: 3440.1176 - val_mean_absolute_error: 44.4335\n",
            "Epoch 41/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2879.1528 - mean_absolute_error: 37.6333 - val_loss: 2959.5275 - val_mean_absolute_error: 41.0369\n",
            "Epoch 42/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2877.1315 - mean_absolute_error: 37.5996 - val_loss: 2909.9695 - val_mean_absolute_error: 40.1506\n",
            "Epoch 43/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2877.9873 - mean_absolute_error: 37.6403 - val_loss: 2744.3862 - val_mean_absolute_error: 37.8476\n",
            "Epoch 44/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2877.9773 - mean_absolute_error: 37.6195 - val_loss: 3043.3528 - val_mean_absolute_error: 42.1354\n",
            "Epoch 45/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2876.8563 - mean_absolute_error: 37.5948 - val_loss: 3186.9469 - val_mean_absolute_error: 41.9379\n",
            "Epoch 46/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2873.8114 - mean_absolute_error: 37.5925 - val_loss: 2950.1931 - val_mean_absolute_error: 39.6768\n",
            "Epoch 47/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2876.3460 - mean_absolute_error: 37.5986 - val_loss: 3017.7258 - val_mean_absolute_error: 41.8365\n",
            "Epoch 48/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2870.2901 - mean_absolute_error: 37.5413 - val_loss: 3101.9604 - val_mean_absolute_error: 42.7929\n",
            "Epoch 49/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2868.8294 - mean_absolute_error: 37.5666 - val_loss: 2924.2485 - val_mean_absolute_error: 41.0054\n",
            "Epoch 50/500\n",
            "152341/152341 [==============================] - 9s 57us/step - loss: 2873.9860 - mean_absolute_error: 37.5416 - val_loss: 3248.5937 - val_mean_absolute_error: 44.3285\n",
            "Epoch 51/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2872.2907 - mean_absolute_error: 37.5774 - val_loss: 2847.4921 - val_mean_absolute_error: 39.8854\n",
            "Epoch 52/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2871.0097 - mean_absolute_error: 37.5471 - val_loss: 3540.9737 - val_mean_absolute_error: 46.7542\n",
            "Epoch 53/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2870.1480 - mean_absolute_error: 37.5547 - val_loss: 3237.6502 - val_mean_absolute_error: 42.1651\n",
            "Epoch 54/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2867.0279 - mean_absolute_error: 37.5768 - val_loss: 3183.4791 - val_mean_absolute_error: 42.3424\n",
            "Epoch 55/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2865.2508 - mean_absolute_error: 37.4924 - val_loss: 3324.3477 - val_mean_absolute_error: 45.6067\n",
            "Epoch 56/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2870.0336 - mean_absolute_error: 37.5291 - val_loss: 3179.1594 - val_mean_absolute_error: 42.6933\n",
            "Epoch 57/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2868.8933 - mean_absolute_error: 37.4801 - val_loss: 2920.1869 - val_mean_absolute_error: 39.8203\n",
            "Epoch 58/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2863.8400 - mean_absolute_error: 37.5078 - val_loss: 3176.8612 - val_mean_absolute_error: 43.9319\n",
            "Epoch 59/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2863.4671 - mean_absolute_error: 37.4903 - val_loss: 2831.9414 - val_mean_absolute_error: 39.4874\n",
            "Epoch 60/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2863.7089 - mean_absolute_error: 37.4825 - val_loss: 3265.6746 - val_mean_absolute_error: 43.7581\n",
            "Epoch 61/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2861.5510 - mean_absolute_error: 37.5193 - val_loss: 2823.7549 - val_mean_absolute_error: 39.0322\n",
            "Epoch 62/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2864.1905 - mean_absolute_error: 37.4954 - val_loss: 2657.2296 - val_mean_absolute_error: 36.9334\n",
            "Epoch 63/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2860.1876 - mean_absolute_error: 37.4528 - val_loss: 3192.6995 - val_mean_absolute_error: 42.4536\n",
            "Epoch 64/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2857.3194 - mean_absolute_error: 37.4940 - val_loss: 3111.4417 - val_mean_absolute_error: 43.1411\n",
            "Epoch 65/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2864.8020 - mean_absolute_error: 37.4768 - val_loss: 2871.7469 - val_mean_absolute_error: 39.6427\n",
            "Epoch 66/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2861.6661 - mean_absolute_error: 37.4742 - val_loss: 3129.2767 - val_mean_absolute_error: 42.8715\n",
            "Epoch 67/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2856.5400 - mean_absolute_error: 37.4127 - val_loss: 2805.3839 - val_mean_absolute_error: 38.9743\n",
            "Epoch 68/500\n",
            "152341/152341 [==============================] - 9s 61us/step - loss: 2859.1585 - mean_absolute_error: 37.4492 - val_loss: 2840.7178 - val_mean_absolute_error: 39.3180\n",
            "Epoch 69/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2854.8668 - mean_absolute_error: 37.4199 - val_loss: 2897.8805 - val_mean_absolute_error: 39.1727\n",
            "Epoch 70/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2853.3092 - mean_absolute_error: 37.3982 - val_loss: 2909.6711 - val_mean_absolute_error: 39.1174\n",
            "Epoch 71/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2857.1575 - mean_absolute_error: 37.4175 - val_loss: 3133.0958 - val_mean_absolute_error: 42.8158\n",
            "Epoch 72/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2862.0909 - mean_absolute_error: 37.4693 - val_loss: 3151.2358 - val_mean_absolute_error: 43.1978\n",
            "Epoch 73/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2860.8548 - mean_absolute_error: 37.4244 - val_loss: 3169.8267 - val_mean_absolute_error: 44.3396\n",
            "Epoch 74/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2863.0955 - mean_absolute_error: 37.4469 - val_loss: 3047.2180 - val_mean_absolute_error: 41.3828\n",
            "Epoch 75/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2863.6710 - mean_absolute_error: 37.4393 - val_loss: 2903.7259 - val_mean_absolute_error: 39.9086\n",
            "Epoch 76/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2860.1591 - mean_absolute_error: 37.4197 - val_loss: 3021.4839 - val_mean_absolute_error: 41.5391\n",
            "Epoch 77/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2858.7703 - mean_absolute_error: 37.3798 - val_loss: 3130.7839 - val_mean_absolute_error: 43.1234\n",
            "Epoch 78/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2859.2202 - mean_absolute_error: 37.3782 - val_loss: 2973.4668 - val_mean_absolute_error: 40.4505\n",
            "Epoch 79/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2855.9812 - mean_absolute_error: 37.3952 - val_loss: 2997.4235 - val_mean_absolute_error: 40.4990\n",
            "Epoch 80/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2855.9651 - mean_absolute_error: 37.3565 - val_loss: 2826.2010 - val_mean_absolute_error: 39.7305\n",
            "Epoch 81/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2858.4983 - mean_absolute_error: 37.3848 - val_loss: 2909.2013 - val_mean_absolute_error: 39.7190\n",
            "Epoch 82/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2856.0581 - mean_absolute_error: 37.3641 - val_loss: 2927.8027 - val_mean_absolute_error: 40.3293\n",
            "Epoch 83/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2859.0262 - mean_absolute_error: 37.3792 - val_loss: 3285.2268 - val_mean_absolute_error: 43.7943\n",
            "Epoch 84/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2856.2927 - mean_absolute_error: 37.3723 - val_loss: 2838.8680 - val_mean_absolute_error: 38.9497\n",
            "Epoch 85/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2856.2399 - mean_absolute_error: 37.3483 - val_loss: 3021.5764 - val_mean_absolute_error: 41.2481\n",
            "Epoch 86/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2858.8524 - mean_absolute_error: 37.3956 - val_loss: 2966.7431 - val_mean_absolute_error: 40.3083\n",
            "Epoch 87/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2857.4273 - mean_absolute_error: 37.3472 - val_loss: 2987.2466 - val_mean_absolute_error: 41.2341\n",
            "Epoch 88/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2853.8681 - mean_absolute_error: 37.3407 - val_loss: 2865.9824 - val_mean_absolute_error: 39.5228\n",
            "Epoch 89/500\n",
            "152341/152341 [==============================] - 9s 57us/step - loss: 2859.8195 - mean_absolute_error: 37.3811 - val_loss: 3196.9328 - val_mean_absolute_error: 43.8315\n",
            "Epoch 90/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2855.2156 - mean_absolute_error: 37.3577 - val_loss: 2804.8433 - val_mean_absolute_error: 38.7270\n",
            "Epoch 91/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2853.5338 - mean_absolute_error: 37.3415 - val_loss: 2966.7038 - val_mean_absolute_error: 40.4014\n",
            "Epoch 92/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2854.9443 - mean_absolute_error: 37.3634 - val_loss: 2814.5591 - val_mean_absolute_error: 38.9403\n",
            "Epoch 93/500\n",
            "152341/152341 [==============================] - 9s 59us/step - loss: 2850.4323 - mean_absolute_error: 37.3475 - val_loss: 3438.2000 - val_mean_absolute_error: 45.2676\n",
            "Epoch 94/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2851.1974 - mean_absolute_error: 37.3288 - val_loss: 2824.5832 - val_mean_absolute_error: 39.0251\n",
            "Epoch 95/500\n",
            "152341/152341 [==============================] - 9s 60us/step - loss: 2856.7107 - mean_absolute_error: 37.3253 - val_loss: 2781.6321 - val_mean_absolute_error: 38.6454\n",
            "Epoch 96/500\n",
            "152341/152341 [==============================] - 9s 57us/step - loss: 2851.7046 - mean_absolute_error: 37.3531 - val_loss: 2856.7913 - val_mean_absolute_error: 39.6727\n",
            "Epoch 97/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2857.4880 - mean_absolute_error: 37.3499 - val_loss: 3294.4259 - val_mean_absolute_error: 44.0105\n",
            "Epoch 98/500\n",
            "152341/152341 [==============================] - 9s 58us/step - loss: 2856.3078 - mean_absolute_error: 37.3683 - val_loss: 3225.2367 - val_mean_absolute_error: 43.3677\n",
            "Epoch 99/500\n",
            "152341/152341 [==============================] - 12s 80us/step - loss: 2855.7640 - mean_absolute_error: 37.3394 - val_loss: 2979.8964 - val_mean_absolute_error: 39.5569\n",
            "Epoch 100/500\n",
            "152341/152341 [==============================] - 9s 61us/step - loss: 2851.0217 - mean_absolute_error: 37.3125 - val_loss: 3131.8198 - val_mean_absolute_error: 41.4283\n",
            "Epoch 101/500\n",
            "152341/152341 [==============================] - 10s 63us/step - loss: 2853.7914 - mean_absolute_error: 37.3239 - val_loss: 2801.4468 - val_mean_absolute_error: 38.6042\n",
            "Epoch 102/500\n",
            "152341/152341 [==============================] - 10s 65us/step - loss: 2849.9318 - mean_absolute_error: 37.3175 - val_loss: 3392.6705 - val_mean_absolute_error: 43.5066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hdTP9_aEFRM",
        "colab_type": "code",
        "outputId": "627029bd-032a-4381-8c46-253a697a110a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Testing on validation set\n",
        "\n",
        "y_pred_nn = model2.predict(test_pd_lm, batch_size=20, verbose=1)\n",
        "\n",
        "result_nn = metrics.mean_absolute_error(y_pred_nn, valid_pd)\n",
        "\n",
        "print(f'The mean absoute error of the Neural Network on the test test is {result_nn}.')\n",
        "\n",
        "score_nn = model_score(valid_pd, y_pred_nn)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_nn)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "707/707 [==============================] - 0s 63us/step\n",
            "The mean absoute error of the Neural Network on the test test is 25.535554069778033.\n",
            "The score of the linear model is: 1236077.2241536288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1vbi_b7jStn",
        "colab_type": "text"
      },
      "source": [
        "# **Model 3.2: Neural Network with Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWy5wW8ujYgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building model\n",
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Dense(100,  activation='relu', input_shape=[len(train_pd_ra.keys())], kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01),  activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model4.add(Dense(1))\n",
        "\n",
        "model4.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model4.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3_P3-oKjnoS",
        "colab_type": "code",
        "outputId": "2b6877eb-b0c6-4ce7-bb90-b590c8cbaf85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "# Fitting Model\n",
        "\n",
        "NNmodel4 = model4.fit(train_pd_ra, y_train_ra, epochs=100, batch_size=64, validation_split=0.05, verbose=1,callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 145605 samples, validate on 7664 samples\n",
            "Epoch 1/100\n",
            "145605/145605 [==============================] - 10s 67us/step - loss: 4796.6725 - mean_absolute_error: 44.6032 - val_loss: 3862.8068 - val_mean_absolute_error: 45.2265\n",
            "Epoch 2/100\n",
            "145605/145605 [==============================] - 9s 60us/step - loss: 3200.0164 - mean_absolute_error: 38.7590 - val_loss: 3336.7090 - val_mean_absolute_error: 42.9633\n",
            "Epoch 3/100\n",
            "145605/145605 [==============================] - 9s 59us/step - loss: 2984.1922 - mean_absolute_error: 37.6988 - val_loss: 2963.6625 - val_mean_absolute_error: 39.5110\n",
            "Epoch 4/100\n",
            "145605/145605 [==============================] - 9s 60us/step - loss: 2888.1090 - mean_absolute_error: 37.1764 - val_loss: 3523.4667 - val_mean_absolute_error: 42.8255\n",
            "Epoch 5/100\n",
            "145605/145605 [==============================] - 9s 59us/step - loss: 2833.2082 - mean_absolute_error: 36.8753 - val_loss: 2816.0754 - val_mean_absolute_error: 38.3688\n",
            "Epoch 6/100\n",
            "145605/145605 [==============================] - 9s 60us/step - loss: 2783.3362 - mean_absolute_error: 36.5347 - val_loss: 3569.5227 - val_mean_absolute_error: 45.8611\n",
            "Epoch 7/100\n",
            "145605/145605 [==============================] - 9s 60us/step - loss: 2748.6395 - mean_absolute_error: 36.3671 - val_loss: 2909.7503 - val_mean_absolute_error: 39.6415\n",
            "Epoch 8/100\n",
            "145605/145605 [==============================] - 9s 59us/step - loss: 2728.3322 - mean_absolute_error: 36.2237 - val_loss: 3972.0447 - val_mean_absolute_error: 49.7576\n",
            "Epoch 9/100\n",
            "145605/145605 [==============================] - 9s 62us/step - loss: 2701.1012 - mean_absolute_error: 36.0068 - val_loss: 3030.0972 - val_mean_absolute_error: 40.5651\n",
            "Epoch 10/100\n",
            "145605/145605 [==============================] - 9s 61us/step - loss: 2689.0528 - mean_absolute_error: 35.9267 - val_loss: 5213.3987 - val_mean_absolute_error: 56.3556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1GzVk_3kLAt",
        "colab_type": "code",
        "outputId": "5f08cb59-c695-4a8b-e980-200c90eb882e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Testing on validation set\n",
        "\n",
        "y_pred_nn2 = model4.predict(test_pd_ra, batch_size=20, verbose=1)\n",
        "\n",
        "result_nn2 = metrics.mean_absolute_error(y_pred_nn2, valid_pd)\n",
        "\n",
        "print(f'The mean absoute error of the Neural Network on the test test is {result_nn2}.')\n",
        "\n",
        "score_nn2 = model_score(valid_pd, y_pred_nn2)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_nn2)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "707/707 [==============================] - 0s 58us/step\n",
            "The mean absoute error of the Neural Network on the test test is 26.99157351516767.\n",
            "The score of the linear model is: 786005.6983353865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4atglFkEHw50",
        "colab_type": "text"
      },
      "source": [
        "# **Model 3.2: Neural Network with LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhFj4atfNtGW",
        "outputId": "581beb80-7c3b-4b22-e5b2-f766777c0506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Using Azure tutorial for predictive maintenance for data manipulation\n",
        "# Reference: https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n",
        "\n",
        "# Picking a sequence length - This will be the window of time in which the LSTM will gather data from\n",
        "\n",
        "sequence_length = 50\n",
        "\n",
        "\n",
        "# This function will reshape our data so it can be usable with Keras (Samples, time window, features)\n",
        "def gen_sequence(id_df, seq_length,seq_cols):\n",
        "  data_array = id_df[seq_cols].values\n",
        "  num_elements = data_array.shape[0]\n",
        "  for start,stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "    yield data_array[start:stop,:]\n",
        "\n",
        "# Reference column names\n",
        "\n",
        "sensor_cols = ['S' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['Time (Cycles)', 'OS1', 'OS2', 'OS3']\n",
        "other_cols = ['Condition (One)', 'Fault ONE']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "sequence_cols.extend(other_cols)\n",
        "\n",
        "# Generating sequences\n",
        "\n",
        "seq_gen = (list(gen_sequence(train_pd_scaled[train_pd_scaled['id']==id], sequence_length,sequence_cols))\n",
        "          for id in train_pd_scaled['id'].unique())\n",
        "\n",
        "# Generate sequences and convert to numpy array\n",
        "\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "  data = id_df[label].values\n",
        "  num_elements = data.shape[0]\n",
        "  return data[seq_length:num_elements,:]\n",
        "\n",
        "label_gen = [gen_labels(train_pd_scaled[train_pd_scaled['id']==id], sequence_length, ['RUL'])\n",
        "            for id in train_pd_scaled['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "print(label_array.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(124909, 50, 27)\n",
            "(124909, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7CzDE0PV9a0",
        "colab_type": "code",
        "outputId": "cce9265d-f340-4063-80f6-ef496eb96459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "# Building the RNN\n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=50, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=nb_out))\n",
        "model.add(Activation('linear'))\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-62d45f022113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnb_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_array' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhCTv_y1dYxz",
        "colab_type": "code",
        "outputId": "64ca1bb0-3d64-47d5-f848-b42498c22a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "# Fitting the RNN\n",
        "\n",
        "model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=1, callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-383b4215c44c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWRJI83J1R67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding RUL to test set\n",
        "\n",
        "truth_nn= valid_pd.copy()\n",
        "max_hid = pd.DataFrame(test_pd.groupby('id')['Time (Cycles)'].max()).reset_index()\n",
        "max_hid.columns = ['id','max']\n",
        "truth_nn.columns = ['truth']\n",
        "truth_nn['id'] = truth_nn.index +1\n",
        "truth_nn['truth'] = truth_nn['truth'] + max_hid['max']\n",
        "\n",
        "test_pd_nn = test_pd.copy()\n",
        "test_pd_nn = test_pd_nn.merge(truth_nn, on=['id'], how='left')\n",
        "test_pd_nn['RUL'] = test_pd_nn['truth'] - test_pd_nn['Time (Cycles)']\n",
        "test_pd_nn.drop('truth', axis=1, inplace=True)\n",
        "\n",
        "print(test_pd_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCjYCkhVvv_i",
        "colab_type": "code",
        "outputId": "d62a469c-5fc3-4f07-b409-363b8d8a347d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Preparing test set\n",
        "\n",
        "seq_array_test = [test_pd_nn[test_pd_nn['id']==id][sequence_cols].values[-sequence_length:]\n",
        "                 for id in test_pd_nn['id'].unique() if len(test_pd_nn[test_pd_nn['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test = np.asarray(seq_array_test).astype(np.float32)\n",
        "\n",
        "y_mask = [len(test_pd_nn[test_pd_nn['id']==id]) >= sequence_length for id in test_pd_nn['id'].unique()]\n",
        "\n",
        "label_array_test = test_pd_nn.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test = label_array_test.reshape(label_array_test.shape[0],1).astype(np.float32)\n",
        "\n",
        "print(seq_array_test.shape)\n",
        "print(label_array_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(653, 50, 27)\n",
            "(653, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27TmEvu6E0lz",
        "colab_type": "code",
        "outputId": "eac19d67-46ce-49b4-ef44-3d4f73ffd986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Fitting model on test set\n",
        "\n",
        "y_pred_lstm = model.predict(seq_array_test, batch_size=32, verbose=1)\n",
        "\n",
        "result_lstm =  metrics.mean_absolute_error(y_pred_lstm, label_array_test)\n",
        "\n",
        "print(f'The mean absolute error for the LSTM on the test set is {result_lstm}.' )\n",
        "\n",
        "score_lstm = model_score(valid_pd, y_pred_lstm)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_lstm)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "653/653 [==============================] - 3s 4ms/step\n",
            "The mean absolute error for the LSTM on the test set is 41.70280838012695.\n",
            "The score of the linear model is: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qca7CTFGvuu4",
        "colab_type": "text"
      },
      "source": [
        "## **Neural Network models with custom loss function**\n",
        "\n",
        "This function will penalize heavily predictions that are greater than the true RUL. In practice, there could be grave consequences to overshooting. Thus, we will penalize by two times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dYuyzLwwI9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def customLossMSE(true,pred):\n",
        "  diff = pred - true\n",
        "\n",
        "  greater= k.greater(diff,0)\n",
        "  greater= k.cast(greater, k.floatx())\n",
        "  greater += 1\n",
        "\n",
        "  return k.mean(greater*k.square(diff))\n",
        "\n",
        "def customLossMAE(true,pred):\n",
        "  diff = pred - true\n",
        "\n",
        "  greater= k.greater(diff,0)\n",
        "  greater= k.cast(greater, k.floatx())\n",
        "  greater += 1\n",
        "\n",
        "  return k.mean(greater*k.absolute(diff))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWuodGoxQJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building NN Model\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(100,  activation='relu', input_shape=[len(train_pd_lm.keys())], kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01),  activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model3.add(Dense(1))\n",
        "\n",
        "model3.compile(loss=customLossMSE, optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model3.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2IMOJV0xtL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting model\n",
        "\n",
        "model3.fit(train_pd_lm, y_train, epochs=100, batch_size=64, validation_split=0.05, verbose=1,callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVuEbm2obqUy",
        "colab_type": "code",
        "outputId": "3040d1df-d0bf-4cb8-e616-0cf3bbb91bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Testing on validation set\n",
        "\n",
        "y_pred_nn_cus = model3.predict(test_pd_lm, batch_size=20, verbose=1)\n",
        "\n",
        "result_nn_cus = metrics.mean_absolute_error(y_pred_nn_cus, valid_pd)\n",
        "\n",
        "print(f'The mean absoute error of the Neural Network on the test test is {result_nn_cus}.')\n",
        "\n",
        "score_nn_cus = model_score(valid_pd, y_pred_nn_cus)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_nn_cus)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "707/707 [==============================] - 2s 2ms/step\n",
            "The mean absoute error of the Neural Network on the test test is 24.387866405988852.\n",
            "The score of the linear model is: 350476.33434362814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oskkZ-QwovnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building model with feature engineering\n",
        "\n",
        "model5 = Sequential()\n",
        "model5.add(Dense(100,  activation='relu', input_shape=[len(train_pd_ra.keys())], kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01),  activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "model5.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model5.add(Dense(1))\n",
        "\n",
        "model5.compile(loss=customLossMSE, optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model5.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYdKd6QLo7KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting Model\n",
        "\n",
        "model5.fit(train_pd_ra, y_train_ra, epochs=100, batch_size=64, validation_split=0.05, verbose=1,callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVMMMEcvpAEU",
        "colab_type": "code",
        "outputId": "9cc8d7de-4416-49b0-abe5-8a93bf0decf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Testing on validation set\n",
        "\n",
        "y_pred_nn_cus_ra = model5.predict(test_pd_ra, batch_size=20, verbose=1)\n",
        "\n",
        "result_nn_cus_ra = metrics.mean_absolute_error(y_pred_nn_cus_ra, valid_pd)\n",
        "\n",
        "print(f'The mean absoute error of the Neural Network on the test test is {result_nn_cus_ra}.')\n",
        "\n",
        "score_nn_cus_ra = model_score(valid_pd, y_pred_nn_cus_ra)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_nn_cus_ra)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "707/707 [==============================] - 0s 70us/step\n",
            "The mean absoute error of the Neural Network on the test test is 26.103144309476882.\n",
            "The score of the linear model is: 1552295.2335257854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeg44qbqyFuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building LSTM model\n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model6 = Sequential()\n",
        "model6.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model6.add(Dropout(0.2))\n",
        "model6.add(LSTM(input_shape=(sequence_length, nb_features), units=50, return_sequences=False))\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "model6.add(Dense(units=nb_out))\n",
        "model6.add(Activation('relu'))\n",
        "model6.compile(loss=customLossMSE, optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model6.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSlCPlX9yj2p",
        "colab_type": "code",
        "outputId": "f37d2b3d-deca-47b4-f6a8-31ee81fe2106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "model6.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=1, callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 118663 samples, validate on 6246 samples\n",
            "Epoch 1/100\n",
            "118663/118663 [==============================] - 162s 1ms/step - loss: 10860.0463 - mean_absolute_error: 77.5889 - val_loss: 9151.1605 - val_mean_absolute_error: 72.0223\n",
            "Epoch 2/100\n",
            "118663/118663 [==============================] - 162s 1ms/step - loss: 7648.4114 - mean_absolute_error: 61.5949 - val_loss: 6660.0211 - val_mean_absolute_error: 59.4159\n",
            "Epoch 3/100\n",
            "118663/118663 [==============================] - 162s 1ms/step - loss: 5770.0928 - mean_absolute_error: 51.1364 - val_loss: 4469.0203 - val_mean_absolute_error: 46.4810\n",
            "Epoch 4/100\n",
            "118663/118663 [==============================] - 162s 1ms/step - loss: 4609.0389 - mean_absolute_error: 44.1671 - val_loss: 3513.7253 - val_mean_absolute_error: 39.9294\n",
            "Epoch 5/100\n",
            "118663/118663 [==============================] - 162s 1ms/step - loss: 4000.2422 - mean_absolute_error: 40.4408 - val_loss: 2867.8935 - val_mean_absolute_error: 35.5230\n",
            "Epoch 6/100\n",
            "118663/118663 [==============================] - 162s 1ms/step - loss: 3590.9032 - mean_absolute_error: 37.9179 - val_loss: 4796.6242 - val_mean_absolute_error: 50.8609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f04b94277f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQXnITrirNq4",
        "colab_type": "code",
        "outputId": "84c79e19-4069-4fa4-c9df-9d42f120ae43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Fitting LSTM with custom loss\n",
        "\n",
        "y_pred_lstm_cus = model6.predict(seq_array_test, batch_size=32, verbose=1)\n",
        "\n",
        "result_lstm_cus =  metrics.mean_absolute_error(y_pred_lstm_cus, label_array_test)\n",
        "\n",
        "print(f'The mean absolute error for the LSTM on the test set is {result_lstm_cus}.' )\n",
        "\n",
        "score_lstm_cus = model_score(valid_pd, y_pred_lstm_cus)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_lstm_cus)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "653/653 [==============================] - 0s 628us/step\n",
            "The mean absolute error for the LSTM on the test set is 43.24674987792969.\n",
            "The score of the linear model is: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TseUOYQNvB-k",
        "colab_type": "code",
        "outputId": "0a916ab1-141d-4d25-e56b-013408ff27e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(y_pred_lstm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.76844 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69473 ]\n",
            " [84.69474 ]\n",
            " [84.670044]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [74.07733 ]\n",
            " [73.30086 ]\n",
            " [73.99956 ]\n",
            " [74.070625]\n",
            " [73.73381 ]\n",
            " [74.79967 ]\n",
            " [74.187195]\n",
            " [73.87186 ]\n",
            " [73.249695]\n",
            " [74.01689 ]\n",
            " [73.84964 ]\n",
            " [74.03496 ]\n",
            " [73.94568 ]\n",
            " [73.89455 ]\n",
            " [73.711624]\n",
            " [73.68001 ]\n",
            " [74.10605 ]\n",
            " [73.69284 ]\n",
            " [74.549995]\n",
            " [74.075005]\n",
            " [73.62789 ]\n",
            " [73.544014]\n",
            " [73.84014 ]\n",
            " [73.6447  ]\n",
            " [73.19277 ]\n",
            " [74.43608 ]\n",
            " [74.32394 ]\n",
            " [73.50814 ]\n",
            " [72.79053 ]\n",
            " [74.64833 ]\n",
            " [74.65992 ]\n",
            " [73.73777 ]\n",
            " [74.295715]\n",
            " [73.71969 ]\n",
            " [73.63571 ]\n",
            " [73.85216 ]\n",
            " [74.1661  ]\n",
            " [73.53319 ]\n",
            " [75.1576  ]\n",
            " [72.61182 ]\n",
            " [73.72299 ]\n",
            " [73.64066 ]\n",
            " [74.061035]\n",
            " [73.23473 ]\n",
            " [73.901505]\n",
            " [73.833755]\n",
            " [73.5048  ]\n",
            " [74.48594 ]\n",
            " [74.412605]\n",
            " [73.7009  ]\n",
            " [74.282974]\n",
            " [74.06423 ]\n",
            " [74.50825 ]\n",
            " [74.57064 ]\n",
            " [73.980194]\n",
            " [74.37735 ]\n",
            " [73.42202 ]\n",
            " [74.64565 ]\n",
            " [74.68663 ]\n",
            " [73.42697 ]\n",
            " [74.00252 ]\n",
            " [74.14618 ]\n",
            " [73.483574]\n",
            " [72.95671 ]\n",
            " [73.39483 ]\n",
            " [73.773445]\n",
            " [73.622154]\n",
            " [74.75877 ]\n",
            " [74.15848 ]\n",
            " [73.6498  ]\n",
            " [74.1478  ]\n",
            " [73.62478 ]\n",
            " [74.64082 ]\n",
            " [74.95716 ]\n",
            " [74.38733 ]\n",
            " [73.53872 ]\n",
            " [73.89303 ]\n",
            " [73.46302 ]\n",
            " [73.97817 ]\n",
            " [72.904724]\n",
            " [74.1296  ]\n",
            " [73.818665]\n",
            " [73.779785]\n",
            " [73.550125]\n",
            " [73.44183 ]\n",
            " [73.29863 ]\n",
            " [73.63075 ]\n",
            " [73.605995]\n",
            " [73.66202 ]\n",
            " [73.736885]\n",
            " [73.86395 ]\n",
            " [73.41337 ]\n",
            " [73.21023 ]\n",
            " [73.296074]\n",
            " [74.10842 ]\n",
            " [74.06277 ]\n",
            " [73.21659 ]\n",
            " [74.03683 ]\n",
            " [74.42131 ]\n",
            " [73.85495 ]\n",
            " [73.57412 ]\n",
            " [74.476906]\n",
            " [73.31607 ]\n",
            " [74.17715 ]\n",
            " [73.37905 ]\n",
            " [73.69239 ]\n",
            " [74.0585  ]\n",
            " [74.125046]\n",
            " [73.89796 ]\n",
            " [74.07636 ]\n",
            " [73.51546 ]\n",
            " [73.994736]\n",
            " [73.77263 ]\n",
            " [73.68539 ]\n",
            " [73.30389 ]\n",
            " [73.81461 ]\n",
            " [74.185036]\n",
            " [73.363556]\n",
            " [73.71564 ]\n",
            " [74.12381 ]\n",
            " [73.85186 ]\n",
            " [73.402   ]\n",
            " [74.22903 ]\n",
            " [74.12894 ]\n",
            " [72.97839 ]\n",
            " [74.160805]\n",
            " [74.79696 ]\n",
            " [74.261925]\n",
            " [73.90461 ]\n",
            " [75.1239  ]\n",
            " [74.99451 ]\n",
            " [73.08719 ]\n",
            " [74.15088 ]\n",
            " [73.40615 ]\n",
            " [73.99769 ]\n",
            " [73.73607 ]\n",
            " [73.51145 ]\n",
            " [74.34452 ]\n",
            " [74.1126  ]\n",
            " [73.53761 ]\n",
            " [74.04558 ]\n",
            " [74.094894]\n",
            " [73.72315 ]\n",
            " [73.97639 ]\n",
            " [74.13258 ]\n",
            " [73.57819 ]\n",
            " [74.42946 ]\n",
            " [73.063484]\n",
            " [73.90604 ]\n",
            " [73.49827 ]\n",
            " [73.44298 ]\n",
            " [73.12449 ]\n",
            " [73.49936 ]\n",
            " [73.17445 ]\n",
            " [73.75234 ]\n",
            " [73.858696]\n",
            " [74.19995 ]\n",
            " [73.638374]\n",
            " [74.07574 ]\n",
            " [74.90816 ]\n",
            " [73.97686 ]\n",
            " [73.45096 ]\n",
            " [73.0289  ]\n",
            " [73.654655]\n",
            " [74.1848  ]\n",
            " [74.2433  ]\n",
            " [73.90776 ]\n",
            " [74.367714]\n",
            " [73.8369  ]\n",
            " [74.816414]\n",
            " [73.863174]\n",
            " [74.166855]\n",
            " [73.65126 ]\n",
            " [73.134514]\n",
            " [73.35226 ]\n",
            " [74.13373 ]\n",
            " [73.866875]\n",
            " [74.47585 ]\n",
            " [73.75774 ]\n",
            " [74.15311 ]\n",
            " [73.54027 ]\n",
            " [74.36572 ]\n",
            " [74.087074]\n",
            " [73.69856 ]\n",
            " [73.965096]\n",
            " [73.00443 ]\n",
            " [73.40845 ]\n",
            " [74.04184 ]\n",
            " [74.241455]\n",
            " [74.06287 ]\n",
            " [72.79539 ]\n",
            " [73.524666]\n",
            " [74.40052 ]\n",
            " [73.65032 ]\n",
            " [73.049614]\n",
            " [74.56034 ]\n",
            " [73.75825 ]\n",
            " [73.20454 ]\n",
            " [73.984634]\n",
            " [74.29969 ]\n",
            " [74.3898  ]\n",
            " [73.627174]\n",
            " [73.35605 ]\n",
            " [74.1155  ]\n",
            " [73.75154 ]\n",
            " [74.03482 ]\n",
            " [73.050255]\n",
            " [73.9524  ]\n",
            " [74.57231 ]\n",
            " [74.37185 ]\n",
            " [74.19062 ]\n",
            " [74.1847  ]\n",
            " [74.24969 ]\n",
            " [74.362595]\n",
            " [73.63128 ]\n",
            " [73.24096 ]\n",
            " [74.495995]\n",
            " [74.1238  ]\n",
            " [74.14596 ]\n",
            " [74.53798 ]\n",
            " [73.51826 ]\n",
            " [73.25967 ]\n",
            " [74.05257 ]\n",
            " [74.404785]\n",
            " [73.32762 ]\n",
            " [74.84757 ]\n",
            " [73.9917  ]\n",
            " [74.919525]\n",
            " [74.51922 ]\n",
            " [73.860565]\n",
            " [74.80334 ]\n",
            " [72.89596 ]\n",
            " [74.27135 ]\n",
            " [73.620186]\n",
            " [74.01211 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.653   ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.15929 ]\n",
            " [84.69474 ]\n",
            " [88.76845 ]\n",
            " [88.76845 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [86.42695 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.76845 ]\n",
            " [86.55733 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69473 ]\n",
            " [84.69474 ]\n",
            " [86.93042 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.76845 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.76845 ]\n",
            " [88.76764 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.83661 ]\n",
            " [84.69474 ]\n",
            " [88.76845 ]\n",
            " [84.69474 ]\n",
            " [88.76845 ]\n",
            " [84.69474 ]\n",
            " [84.69474 ]\n",
            " [88.76843 ]\n",
            " [83.93369 ]\n",
            " [74.5462  ]\n",
            " [73.86494 ]\n",
            " [74.64238 ]\n",
            " [73.86455 ]\n",
            " [73.46842 ]\n",
            " [74.24645 ]\n",
            " [73.19178 ]\n",
            " [74.2954  ]\n",
            " [74.0672  ]\n",
            " [74.59018 ]\n",
            " [74.599144]\n",
            " [74.32973 ]\n",
            " [74.28627 ]\n",
            " [73.52053 ]\n",
            " [74.09224 ]\n",
            " [75.33743 ]\n",
            " [74.509315]\n",
            " [73.75817 ]\n",
            " [73.83179 ]\n",
            " [72.86728 ]\n",
            " [73.60837 ]\n",
            " [74.222305]\n",
            " [71.285515]\n",
            " [74.029655]\n",
            " [74.05934 ]\n",
            " [74.30629 ]\n",
            " [74.625534]\n",
            " [73.530205]\n",
            " [74.43766 ]\n",
            " [73.68649 ]\n",
            " [75.03861 ]\n",
            " [74.27937 ]\n",
            " [73.45303 ]\n",
            " [74.64317 ]\n",
            " [73.902695]\n",
            " [74.342064]\n",
            " [74.71689 ]\n",
            " [73.35884 ]\n",
            " [74.111305]\n",
            " [74.11647 ]\n",
            " [73.877716]\n",
            " [73.09083 ]\n",
            " [74.02664 ]\n",
            " [74.733795]\n",
            " [74.65597 ]\n",
            " [74.07539 ]\n",
            " [73.90787 ]\n",
            " [73.89643 ]\n",
            " [73.53334 ]\n",
            " [73.95373 ]\n",
            " [73.306625]\n",
            " [73.93037 ]\n",
            " [73.62155 ]\n",
            " [74.477806]\n",
            " [73.481926]\n",
            " [73.36017 ]\n",
            " [74.85815 ]\n",
            " [73.261734]\n",
            " [74.19367 ]\n",
            " [73.718414]\n",
            " [74.185486]\n",
            " [74.024376]\n",
            " [73.86908 ]\n",
            " [74.02525 ]\n",
            " [74.377686]\n",
            " [73.990814]\n",
            " [73.42358 ]\n",
            " [73.96844 ]\n",
            " [74.92142 ]\n",
            " [74.24804 ]\n",
            " [74.02952 ]\n",
            " [74.20517 ]\n",
            " [73.601074]\n",
            " [74.57726 ]\n",
            " [73.6108  ]\n",
            " [73.93084 ]\n",
            " [74.786285]\n",
            " [73.75367 ]\n",
            " [74.322624]\n",
            " [74.501175]\n",
            " [74.22905 ]\n",
            " [74.28824 ]\n",
            " [73.30096 ]\n",
            " [73.70375 ]\n",
            " [73.76412 ]\n",
            " [74.693   ]\n",
            " [74.50727 ]\n",
            " [72.7071  ]\n",
            " [73.1646  ]\n",
            " [74.235565]\n",
            " [73.818474]\n",
            " [74.51751 ]\n",
            " [74.05425 ]\n",
            " [74.3037  ]\n",
            " [72.75858 ]\n",
            " [72.4737  ]\n",
            " [73.80945 ]\n",
            " [74.23344 ]\n",
            " [75.36988 ]\n",
            " [73.807625]\n",
            " [73.30731 ]\n",
            " [74.002754]\n",
            " [73.483795]\n",
            " [70.88165 ]\n",
            " [75.18015 ]\n",
            " [73.30037 ]\n",
            " [73.42974 ]\n",
            " [74.581955]\n",
            " [73.82639 ]\n",
            " [73.49308 ]\n",
            " [73.51329 ]\n",
            " [73.751274]\n",
            " [74.21582 ]\n",
            " [73.86991 ]\n",
            " [73.97027 ]\n",
            " [74.35733 ]\n",
            " [74.76654 ]\n",
            " [74.13576 ]\n",
            " [74.37178 ]\n",
            " [73.31627 ]\n",
            " [74.251144]\n",
            " [74.412415]\n",
            " [74.020836]\n",
            " [73.71078 ]\n",
            " [73.81973 ]\n",
            " [73.99819 ]\n",
            " [74.886406]\n",
            " [74.38453 ]\n",
            " [73.7872  ]\n",
            " [73.9528  ]\n",
            " [74.94532 ]\n",
            " [73.408264]\n",
            " [74.2883  ]\n",
            " [74.04832 ]\n",
            " [73.37088 ]\n",
            " [74.086136]\n",
            " [74.16131 ]\n",
            " [74.04398 ]\n",
            " [73.75364 ]\n",
            " [76.16427 ]\n",
            " [73.77906 ]\n",
            " [73.78598 ]\n",
            " [73.886375]\n",
            " [74.209564]\n",
            " [74.198105]\n",
            " [73.022644]\n",
            " [72.87608 ]\n",
            " [73.58119 ]\n",
            " [74.207596]\n",
            " [74.36356 ]\n",
            " [74.97659 ]\n",
            " [73.94766 ]\n",
            " [73.78651 ]\n",
            " [73.55209 ]\n",
            " [73.84285 ]\n",
            " [74.36482 ]\n",
            " [74.2041  ]\n",
            " [74.63221 ]\n",
            " [73.65192 ]\n",
            " [74.17151 ]\n",
            " [74.09519 ]\n",
            " [72.93365 ]\n",
            " [74.55814 ]\n",
            " [74.40077 ]\n",
            " [74.05184 ]\n",
            " [72.30552 ]\n",
            " [74.74181 ]\n",
            " [73.10979 ]\n",
            " [74.79343 ]\n",
            " [73.8731  ]\n",
            " [73.895164]\n",
            " [73.59969 ]\n",
            " [74.7368  ]\n",
            " [74.08449 ]\n",
            " [73.606125]\n",
            " [73.606766]\n",
            " [73.85832 ]\n",
            " [73.90925 ]\n",
            " [73.85751 ]\n",
            " [74.32213 ]\n",
            " [73.90015 ]\n",
            " [73.95889 ]\n",
            " [74.35182 ]\n",
            " [73.64309 ]\n",
            " [73.73859 ]\n",
            " [73.92114 ]\n",
            " [74.00337 ]\n",
            " [74.75329 ]\n",
            " [74.05123 ]\n",
            " [73.08304 ]\n",
            " [72.50665 ]\n",
            " [74.18032 ]\n",
            " [73.624275]\n",
            " [72.79842 ]\n",
            " [73.74068 ]\n",
            " [73.36084 ]\n",
            " [73.775055]\n",
            " [71.235016]\n",
            " [74.37161 ]\n",
            " [74.47437 ]\n",
            " [73.29525 ]\n",
            " [74.04121 ]\n",
            " [74.28991 ]\n",
            " [74.01097 ]\n",
            " [74.09752 ]\n",
            " [73.57635 ]\n",
            " [73.70447 ]\n",
            " [74.74771 ]\n",
            " [73.5682  ]\n",
            " [73.275085]\n",
            " [74.03766 ]\n",
            " [74.14222 ]\n",
            " [74.46129 ]\n",
            " [74.0426  ]\n",
            " [74.428986]\n",
            " [74.4506  ]\n",
            " [74.64768 ]\n",
            " [74.22674 ]\n",
            " [73.4683  ]\n",
            " [74.323044]\n",
            " [73.46643 ]\n",
            " [73.85737 ]\n",
            " [74.2835  ]\n",
            " [74.390175]\n",
            " [74.12041 ]\n",
            " [73.79811 ]\n",
            " [74.29973 ]\n",
            " [74.08269 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnQDqZ7igkYq",
        "colab_type": "text"
      },
      "source": [
        "# **Testing with Softplus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Y56QAzgnZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}