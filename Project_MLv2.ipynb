{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francoisdoanp/MLTBP/blob/master/Project_MLv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "23I2qdWI4FFG"
      },
      "source": [
        "# **Machine learning - Final Project**\n",
        "\n",
        "# Turbofan engine degradation dataset (NASA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nOxT0nXc366_"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "**Importing necessary packages**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J_GboRpW2F6s",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import sklearn.metrics as metrics\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from keras import regularizers\n",
        "import keras.backend as k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qw2HZErj35aM"
      },
      "source": [
        "**Importing the Turbofan engine degradation dataset.**\n",
        "\n",
        "**Files are located in the following Github repository: https://github.com/francoisdoanp/MLTBP**\n",
        "\n",
        "We have 4 training datasets, which contains information about one hundred engines, all of the same type. Thus, we will combine the training and test data sets. \n",
        "\n",
        "The training and test sets have 21 columns: ID, Time (Cycles), 3 columns for operational settings and 21 sensor measurements.\n",
        "\n",
        "The training and testing sets have the same format, while the validation sets only contain the real RUL (remaining useful life).\n",
        "\n",
        "For more information on the data, consult the read me at the following address:https://github.com/francoisdoanp/MLTBP/blob/master/readme.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CmZzDVll4vDW",
        "colab": {}
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/francoisdoanp/MLTBP/master/'\n",
        "\n",
        "file_train_1 = 'train_FD001.txt'\n",
        "file_train_2 = 'train_FD002.txt'\n",
        "file_train_3 = 'train_FD003.txt'\n",
        "file_train_4 = 'train_FD004.txt'\n",
        "\n",
        "file_test_1 = 'test_FD001.txt'\n",
        "file_test_2 = 'test_FD002.txt'\n",
        "file_test_3 = 'test_FD003.txt'\n",
        "file_test_4 = 'test_FD004.txt'\n",
        "\n",
        "file_valid_1 = 'RUL_FD001.txt'\n",
        "file_valid_2 = 'RUL_FD002.txt'\n",
        "file_valid_3 = 'RUL_FD003.txt'\n",
        "file_valid_4 = 'RUL_FD004.txt'\n",
        "\n",
        "\n",
        "pt1 = pd.read_csv(url_base + file_train_1, sep=' ', header=None)\n",
        "pt2 = pd.read_csv(url_base + file_train_2, sep=' ', header=None)\n",
        "pt3 = pd.read_csv(url_base + file_train_3, sep=' ', header=None)\n",
        "pt4 = pd.read_csv(url_base + file_train_4, sep=' ', header=None)\n",
        "\n",
        "pte1 = pd.read_csv(url_base + file_test_1, sep=' ', header=None)\n",
        "pte2 = pd.read_csv(url_base + file_test_2, sep=' ', header=None)\n",
        "pte3 = pd.read_csv(url_base + file_test_3, sep=' ', header=None)\n",
        "pte4 = pd.read_csv(url_base + file_test_4, sep=' ', header=None)\n",
        "\n",
        "pv1 = pd.read_csv(url_base + file_valid_1, header=None)\n",
        "pv2 = pd.read_csv(url_base + file_valid_2, header=None)\n",
        "pv3 = pd.read_csv(url_base + file_valid_3, header=None)\n",
        "pv4 = pd.read_csv(url_base + file_valid_4, header=None)\n",
        "\n",
        "\n",
        "# Updating ids\n",
        "\n",
        "pt2[0] = pt2[0].apply(lambda x: x+100)\n",
        "pt3[0] = pt3[0].apply(lambda x: x+360)\n",
        "pt4[0] = pt4[0].apply(lambda x: x+460)\n",
        "\n",
        "pte2[0] = pte2[0].apply(lambda x: x+100)\n",
        "pte3[0] = pte3[0].apply(lambda x: x+359)\n",
        "pte4[0] = pte4[0].apply(lambda x: x+459)\n",
        "\n",
        "\n",
        "# Joining the dataframes\n",
        "\n",
        "train_pd = pd.concat([pt1,pt2,pt3,pt4]).reset_index(drop=True)\n",
        "test_pd = pd.concat([pte1,pte2,pte3,pte4]).reset_index(drop=True)\n",
        "valid_pd = pd.concat([pv1,pv2,pv3,pv4], ignore_index=True)\n",
        "\n",
        "train_pd = train_pd.drop(train_pd.columns[[26,27]], axis='columns')\n",
        "test_pd = test_pd.drop(test_pd.columns[[26,27]], axis='columns')\n",
        "\n",
        "\n",
        "# Assigning labels to Dataframe's columns based on the Readme\n",
        "\n",
        "train_pd.columns = ['id', 'Time (Cycles)', 'OS1', 'OS2', 'OS3', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21']\n",
        "test_pd.columns = ['id', 'Time (Cycles)', 'OS1', 'OS2', 'OS3', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21']\n",
        "valid_pd.columns = ['RUL']\n",
        "\n",
        "#Loading scaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G9va4qroPgeB"
      },
      "source": [
        "**Adding variables Conditons and fault mode**\n",
        "\n",
        "Note:\n",
        "\n",
        "**Condition (ONE)** and **Fault ONE** are binary variables.\n",
        "\n",
        "When Condition(ONE) = 1 (true), it means that the condition is at Sea Level\n",
        "\n",
        "When Condition(ONE) = 0 (false), it means NO, the condition IS NOT AT SEA LEVEL, and thus is the  second condition; SIX.\n",
        "\n",
        "When Fault ONE = 1 (true), it means that the fault modes is one (HPC Degradation)\n",
        "\n",
        "When Fault ONE = 0 (false), it means that the fault mode is TWO (HPC Degradation and Fan degradation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WQaAK97PfZU",
        "colab": {}
      },
      "source": [
        "# Adding variables Condition and fault modes\n",
        "\n",
        "def value_condition_train(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 360) & (row['id'] > 100):\n",
        "    return 0\n",
        "  elif (row['id'] <= 460) & (row['id'] > 360):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def value_fault_train(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 360) & (row['id'] > 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 460) & (row['id'] > 360):\n",
        "    return 0\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def value_condition_test(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 359) & (row['id'] > 100):\n",
        "    return 0\n",
        "  elif (row['id'] <= 459) & (row['id'] > 359):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def value_fault_test(row):\n",
        "  if (row['id'] <= 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 359) & (row['id'] > 100):\n",
        "    return 1\n",
        "  elif (row['id'] <= 459) & (row['id'] > 359):\n",
        "    return 0\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "train_pd['Condition (One)'] = train_pd.apply(value_condition_train, axis=1)\n",
        "train_pd['Fault ONE'] = train_pd.apply(value_fault_train,axis=1)\n",
        "\n",
        "test_pd['Condition (One)'] = test_pd.apply(value_condition_test, axis=1)\n",
        "test_pd['Fault ONE'] = test_pd.apply(value_fault_test,axis=1)\n",
        "\n",
        "display(train_pd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lAl6DjVwhdhU"
      },
      "source": [
        "At this stage, we create the truth remaining useful life (RUL) for the training set.\n",
        "\n",
        "Important note: In the training set, the last Cycle (represented in the table by 'Time (Cycles)') is when the engine is considered unusable. However, in the test set, the last cycle IS NOT when the engine is considered unusable. It will fail at a later time. Thus, in the valid_pd, we have the true RUL. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UdbuXXApd51H",
        "colab": {}
      },
      "source": [
        "#Adding column for remaining useful life (RUL)\n",
        "\n",
        "y_train = pd.DataFrame(train_pd.groupby(['id'])['Time (Cycles)'].max())\n",
        "\n",
        "train_pd = pd.merge(train_pd,y_train, on='id')\n",
        "train_pd['RUL'] = train_pd['Time (Cycles)_y'] - train_pd['Time (Cycles)_x']\n",
        "train_pd = train_pd.drop('Time (Cycles)_y',1)\n",
        "train_pd = train_pd.rename(columns = {'Time (Cycles)_x':'Time (Cycles)'})\n",
        "\n",
        "y_train = train_pd.iloc[:,28]\n",
        "y_train_id = train_pd.iloc[:,[0,28]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vhz30Roasqsx",
        "colab": {}
      },
      "source": [
        "def model_score(y_true, y_pred):\n",
        "  pred_df = pd.DataFrame(y_pred)\n",
        "  test_err = pd.concat([y_true,pred_df], axis=1, ignore_index=True)\n",
        "  test_err.columns = ['RUL', 'Pred_RUL']\n",
        "  a1 = 10\n",
        "  a2 = 13\n",
        "  score=0\n",
        "\n",
        "  for index, row in test_err.iterrows():\n",
        "    d = row['Pred_RUL'] - row['RUL']\n",
        "    if d < 0:\n",
        "      score += np.expm1(-(d/a1))-1\n",
        "    else:\n",
        "      score += np.expm1(d/a2)-1\n",
        "\n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-mn7nZq39QVV"
      },
      "source": [
        "**Preparing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSvBTo9285u0",
        "colab": {}
      },
      "source": [
        "# Scaling data\n",
        "\n",
        "train_pd_scaled = train_pd.copy()\n",
        "train_pd_scaled.iloc[:,2:26] = scaler.fit_transform(train_pd.iloc[:,2:26])\n",
        "\n",
        "test_pd_scaled = test_pd.copy()\n",
        "test_pd_scaled.iloc[:,2:26] = scaler.fit_transform(test_pd.iloc[:,2:26])\n",
        "\n",
        "pd.set_option('display.max_columns', None)  \n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "print(train_pd_scaled)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOxq5ftzvn5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing RUL and id columns, as we do not want these features to be in our predictors\n",
        "\n",
        "train_pd_ra = train_RA_pd.copy()\n",
        "train_pd_ra = train_pd_ra.drop(['id', 'RUL','Time (Cycles)'],axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ENkana6AH9LA",
        "colab": {}
      },
      "source": [
        "# Removing RUL and id columns, as we do not want these features to be in our predictors\n",
        "\n",
        "train_pd_lm = train_pd_scaled.copy()\n",
        "train_pd_lm = train_pd_lm.drop(['id', 'RUL', 'Time (Cycles)'],axis=1)\n",
        "\n",
        "# Keeping only last time cycle for each id\n",
        "\n",
        "idx = test_pd_scaled.groupby(['id'])['Time (Cycles)'].transform(max) == test_pd_scaled['Time (Cycles)']\n",
        "test_pd_lm = test_pd_scaled[idx]\n",
        "\n",
        "idx4 = test_RA_pd.groupby(['id'])['Time (Cycles)'].transform(max) == test_RA_pd['Time (Cycles)']\n",
        "test_pd_ra = test_RA_pd[idx4]\n",
        "\n",
        "# Removing id column\n",
        "\n",
        "test_pd_lm = test_pd_lm.drop(['id', 'Time (Cycles)'], axis=1)\n",
        "test_pd_ra = test_pd_ra.drop(['id', 'Time (Cycles)'], axis=1)\n",
        "\n",
        "y_train_ra = y_train_id.groupby('id').apply(lambda group: group.iloc[10:]).reset_index(drop=True)\n",
        "y_train_ra = y_train_ra.drop(['id'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4atglFkEHw50",
        "colab_type": "text"
      },
      "source": [
        "# **Model 3.2: Neural Network with LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhFj4atfNtGW",
        "outputId": "581beb80-7c3b-4b22-e5b2-f766777c0506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Using Azure tutorial for predictive maintenance for data manipulation\n",
        "# Reference: https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n",
        "\n",
        "# Picking a sequence length - This will be the window of time in which the LSTM will gather data from\n",
        "\n",
        "sequence_length = 50\n",
        "\n",
        "\n",
        "# This function will reshape our data so it can be usable with Keras (Samples, time window, features)\n",
        "def gen_sequence(id_df, seq_length,seq_cols):\n",
        "  data_array = id_df[seq_cols].values\n",
        "  num_elements = data_array.shape[0]\n",
        "  for start,stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "    yield data_array[start:stop,:]\n",
        "\n",
        "# Reference column names\n",
        "\n",
        "sensor_cols = ['S' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['Time (Cycles)', 'OS1', 'OS2', 'OS3']\n",
        "other_cols = ['Condition (One)', 'Fault ONE']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "sequence_cols.extend(other_cols)\n",
        "\n",
        "# Generating sequences\n",
        "\n",
        "seq_gen = (list(gen_sequence(train_pd_scaled[train_pd_scaled['id']==id], sequence_length,sequence_cols))\n",
        "          for id in train_pd_scaled['id'].unique())\n",
        "\n",
        "# Generate sequences and convert to numpy array\n",
        "\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "  data = id_df[label].values\n",
        "  num_elements = data.shape[0]\n",
        "  return data[seq_length:num_elements,:]\n",
        "\n",
        "label_gen = [gen_labels(train_pd_scaled[train_pd_scaled['id']==id], sequence_length, ['RUL'])\n",
        "            for id in train_pd_scaled['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "print(label_array.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(124909, 50, 27)\n",
            "(124909, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7CzDE0PV9a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the RNN\n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(input_shape=(sequence_length, nb_features), units=50, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=nb_out))\n",
        "model.add(Activation('linear'))\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhCTv_y1dYxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the RNN\n",
        "\n",
        "model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=1, callbacks =[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWRJI83J1R67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding RUL to test set\n",
        "\n",
        "truth_nn= valid_pd.copy()\n",
        "max_hid = pd.DataFrame(test_pd.groupby('id')['Time (Cycles)'].max()).reset_index()\n",
        "max_hid.columns = ['id','max']\n",
        "truth_nn.columns = ['truth']\n",
        "truth_nn['id'] = truth_nn.index +1\n",
        "truth_nn['truth'] = truth_nn['truth'] + max_hid['max']\n",
        "\n",
        "test_pd_nn = test_pd.copy()\n",
        "test_pd_nn = test_pd_nn.merge(truth_nn, on=['id'], how='left')\n",
        "test_pd_nn['RUL'] = test_pd_nn['truth'] - test_pd_nn['Time (Cycles)']\n",
        "test_pd_nn.drop('truth', axis=1, inplace=True)\n",
        "\n",
        "print(test_pd_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCjYCkhVvv_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing test set\n",
        "\n",
        "seq_array_test = [test_pd_nn[test_pd_nn['id']==id][sequence_cols].values[-sequence_length:]\n",
        "                 for id in test_pd_nn['id'].unique() if len(test_pd_nn[test_pd_nn['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test = np.asarray(seq_array_test).astype(np.float32)\n",
        "\n",
        "y_mask = [len(test_pd_nn[test_pd_nn['id']==id]) >= sequence_length for id in test_pd_nn['id'].unique()]\n",
        "\n",
        "label_array_test = test_pd_nn.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test = label_array_test.reshape(label_array_test.shape[0],1).astype(np.float32)\n",
        "\n",
        "print(seq_array_test.shape)\n",
        "print(label_array_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27TmEvu6E0lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting model on test set\n",
        "\n",
        "y_pred_lstm = model.predict(seq_array_test, batch_size=32, verbose=1)\n",
        "\n",
        "result_lstm =  metrics.mean_absolute_error(y_pred_lstm, label_array_test)\n",
        "\n",
        "print(f'The mean absolute error for the LSTM on the test set is {result_lstm}.' )\n",
        "\n",
        "score_lstm = model_score(valid_pd, y_pred_lstm)\n",
        "\n",
        "print(f'The score of the linear model is: {(score_lstm)}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Y56QAzgnZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}